{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9236aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Tuple\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a159d4",
   "metadata": {},
   "source": [
    "Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "684bf2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = load_dataset(\"ccdv/govreport-summarization\")\n",
    "ds2 = load_dataset(\"FiscalNote/billsum\")\n",
    "\n",
    "# GovReport splits (report, summary)\n",
    "gov_train = ds1[\"train\"].rename_columns({\"report\": \"text\"})\n",
    "gov_val   = ds1[\"validation\"].rename_columns({\"report\": \"text\"})\n",
    "gov_test  = ds1[\"test\"].rename_columns({\"report\": \"text\"})\n",
    "\n",
    "# BillSum splits (text, summary, title)\n",
    "bill_train = ds2[\"train\"]\n",
    "bill_test  = ds2[\"test\"]\n",
    "bill_ca_test = ds2[\"ca_test\"]  # treat as out of domain test for model generalisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f5ed8",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03f7fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove boilerplate for text\n",
    "\n",
    "header_irrelevant = [\n",
    "    r\"^Page\\s+\\d+(\\s+of\\s+\\d+)?\\s*$\", # pages e.g. Page 3\n",
    "    r\"^\\d+\\s*$\", # numbers e.g. 13\n",
    "    r\"^–\\s*\\d+\\s*–$\", # e.g. - 12 -\n",
    "    r\"^\\s*U\\.S\\. Government Accountability Office.*$\", # common report headers\n",
    "    r\"^\\s*Congressional Research Service.*$\",\n",
    "    r\"^\\s*Congressional Budget Office.*$\",\n",
    "    r\"^\\s*GAO-\\d{2}-\\d+\\s*$\", # report ids\n",
    "    r\"^\\s*For Official Use Only\\s*$\",\n",
    "    r\"^\\s*This report was prepared by.*$\",\n",
    "]\n",
    "\n",
    "toc_irrelevant = [\n",
    "    r\"^\\s*Table of Contents\\s*$\",\n",
    "    r\"\\.{2,}\\s*\\d+\\s*$\", # dotted lines with page numbers\n",
    "]\n",
    "\n",
    "# remove excessive whitespaces, line breaks\n",
    "def remove_whitespaces(text):\n",
    "    text = re.sub(r'\\r\\n', '\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text) # multiple spaces into single space\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text) # 3+ consecutive newlines into two to keep paragraph breaks\n",
    "    return text.strip() # drop leading/trailing whitespaces\n",
    "\n",
    "def remove_boilerplate(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # standardize unicode/punctuation for tokenizer consistency\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "\n",
    "    inside_toc = False\n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "\n",
    "        if any(re.match(i, line_stripped) for i in header_irrelevant):\n",
    "            continue\n",
    "\n",
    "        if any(re.match(i, line_stripped) for i in toc_irrelevant):\n",
    "            inside_toc = True\n",
    "            continue\n",
    "\n",
    "        if inside_toc:\n",
    "            # skip TOC lines till normal paragraph appears\n",
    "            if re.search(r\"\\.{2,}\\s*\\d+\\s*$\", line_stripped):\n",
    "                continue\n",
    "            inside_toc = False\n",
    "\n",
    "        cleaned.append(line)\n",
    "\n",
    "    cleaned_text = \"\\n\".join(cleaned)\n",
    "    cleaned_text = remove_whitespaces(cleaned_text)\n",
    "    # remove rule lines ------ / =====\n",
    "    cleaned_text = re.sub(r\"(?:^|\\n)[\\-=]{4,}(?:\\n|$)\", \"\\n\", cleaned_text)\n",
    "    # remove HTML tags\n",
    "    tags = re.compile(r\"<[^>]+>|&[a-zA-Z]+;\")\n",
    "    if isinstance(cleaned_text, str) and tags.search(cleaned_text):\n",
    "        cleaned_text = BeautifulSoup(cleaned_text, \"lxml\").get_text(separator=\" \")\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "gov_train = gov_train.map(lambda ex: {\"clean_text\": remove_boilerplate(ex[\"text\"])})\n",
    "gov_val = gov_val.map(lambda ex: {\"clean_text\": remove_boilerplate(ex[\"text\"])})\n",
    "gov_test = gov_test.map(lambda ex: {\"clean_text\": remove_boilerplate(ex[\"text\"])})\n",
    "\n",
    "bill_train = bill_train.map(lambda ex: {\"clean_text\": remove_boilerplate(ex[\"text\"])})\n",
    "bill_test = bill_test.map(lambda ex: {\"clean_text\": remove_boilerplate(ex[\"text\"])})\n",
    "bill_ca_test = bill_ca_test.map(lambda ex: {\"clean_text\": remove_boilerplate(ex[\"text\"])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78082c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove minimal whitespaces for summary\n",
    "def clean_summary(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "gov_train = gov_train.map(lambda ex: {\"clean_summary\": clean_summary(ex[\"summary\"])})\n",
    "gov_val   = gov_val.map(lambda ex: {\"clean_summary\": clean_summary(ex[\"summary\"])})\n",
    "gov_test  = gov_test.map(lambda ex: {\"clean_summary\": clean_summary(ex[\"summary\"])})\n",
    "\n",
    "bill_train = bill_train.map(lambda ex: {\"clean_summary\": clean_summary(ex[\"summary\"])})\n",
    "bill_test  = bill_test.map(lambda ex: {\"clean_summary\": clean_summary(ex[\"summary\"])})\n",
    "bill_ca_test   = bill_ca_test.map(lambda ex: {\"clean_summary\": clean_summary(ex[\"summary\"])})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705980df",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e4fa15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 17517/17517 [02:12<00:00, 132.55 examples/s]\n",
      "Map: 100%|██████████| 973/973 [00:05<00:00, 169.57 examples/s]\n",
      "Map: 100%|██████████| 973/973 [00:05<00:00, 187.35 examples/s]\n",
      "Map: 100%|██████████| 18949/18949 [00:18<00:00, 998.36 examples/s] \n",
      "Map: 100%|██████████| 3269/3269 [00:03<00:00, 967.79 examples/s] \n",
      "Map: 100%|██████████| 1237/1237 [00:01<00:00, 923.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# add sentences column for extractive baselines\n",
    "\n",
    "def sent_split(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    sents = sent_tokenize(text)\n",
    "\n",
    "    # merge headings with next sentence\n",
    "    output = []\n",
    "    heading = None\n",
    "    for s in sents:\n",
    "        s = s.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if heading is None and (len(s) <= 25 and (s.endswith(\":\") or re.match(r\"^[A-Z][A-Za-z0-9 \\-]{0,20}$\", s))):\n",
    "            heading = s\n",
    "            continue\n",
    "        if heading is not None:\n",
    "            output.append((heading + \" \" + s).strip())\n",
    "            heading = None\n",
    "        else:\n",
    "            output.append(s)\n",
    "    if heading is not None:\n",
    "        output.append(heading)\n",
    "    return output\n",
    "\n",
    "gov_train = gov_train.map(lambda ex: {\"sentences\": sent_split(ex[\"clean_text\"])},batched=False)\n",
    "gov_val = gov_val.map(lambda ex: {\"sentences\": sent_split(ex[\"clean_text\"])},batched=False)\n",
    "gov_test = gov_test.map(lambda ex: {\"sentences\": sent_split(ex[\"clean_text\"])},batched=False)\n",
    "\n",
    "bill_train = bill_train.map(lambda ex: {\"sentences\": sent_split(ex[\"clean_text\"])},batched=False)\n",
    "bill_test = bill_test.map(lambda ex: {\"sentences\": sent_split(ex[\"clean_text\"])},batched=False)\n",
    "bill_ca_test = bill_ca_test.map(lambda ex: {\"sentences\": sent_split(ex[\"clean_text\"])},batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f20025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 2607/2607 [00:00<00:00, 4962.17 examples/s]\n",
      "Flattening the indices: 100%|██████████| 2608/2608 [00:00<00:00, 7321.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# dataset splits for modeling\n",
    "split = bill_test.train_test_split(test_size=0.5, seed=42)\n",
    "bill_val = split[\"train\"]\n",
    "bill_test = split[\"test\"]\n",
    "\n",
    "# Combined\n",
    "comb_train = concatenate_datasets([gov_train, bill_train.select_columns(['text', 'summary', 'clean_text', 'clean_summary', 'sentences'])])\n",
    "comb_val = concatenate_datasets([gov_val, bill_val.select_columns(['text', 'summary', 'clean_text', 'clean_summary', 'sentences'])])\n",
    "comb_test = concatenate_datasets([gov_test, bill_test.select_columns(['text', 'summary', 'clean_text', 'clean_summary', 'sentences'])])\n",
    "# add source column\n",
    "comb_train = comb_train.add_column(\"source\",[\"govreport\"] * len(gov_train) + [\"billsum\"] * len(bill_train))\n",
    "comb_val = comb_val.add_column(\"source\",[\"govreport\"] * len(gov_val) + [\"billsum\"] * len(bill_val))\n",
    "comb_test = comb_test.add_column(\"source\",[\"govreport\"] * len(gov_test) + [\"billsum\"] * len(bill_test))\n",
    "# shuffle to avoid bias\n",
    "comb_train = comb_train.shuffle(seed=42)\n",
    "comb_val = comb_val.shuffle(seed=42)\n",
    "comb_test = comb_test.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9497f",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae2fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7995fd5b",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50332fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84d8e7ba",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46000b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
